{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considering our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial goal was to apply a ML approach to accurately predict the likelihood of a wildfire occuring. The data we used was first balanced so we had an equal amount of data for both occasions with and without fires. This data was stored in CSV format. Using the Python library Pandas we can analyse the structure of this CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10341, 23)\n",
      "\n",
      "\n",
      "  CurrentDate  Temp1  Temp2  Temp3  Temp4  avgTemp  Hum1  Hum2  Hum3  Hum4  \\\n",
      "0  01/13/2011     24     16     13     24    19.25    35    32    34    57   \n",
      "1  01/15/2011     28     18     14     25    21.25    64    40    20    45   \n",
      "2  01/20/2011     18     12     22     23    18.75    90    69    18    32   \n",
      "3  01/28/2011     13     11     17     19    15.00    43    26    27    84   \n",
      "4  02/04/2011      8     17     18     13    14.00    70    57    56    84   \n",
      "\n",
      "   ...   Wind3  Wind4   avgWind  Rain  14DayAvgTemp  14dayAvgHum  \\\n",
      "0  ...   1.864  0.000  1.242667     0     14.230769    53.916667   \n",
      "1  ...   0.621  0.000  0.621000     0     15.392857    46.059524   \n",
      "2  ...   0.621  0.000  0.621000     0     18.160714    52.333333   \n",
      "3  ...   0.621  0.000  1.242500     0     19.125000    46.107143   \n",
      "4  ...   2.486  0.621  1.553500     0     16.160714    64.142857   \n",
      "\n",
      "   14DayAvgWind  14DayAvgRain              Location  Fire  \n",
      "0             2             0  La Ca_ada Flintridge     1  \n",
      "1             2             0  La Ca_ada Flintridge     1  \n",
      "2             1             0  La Ca_ada Flintridge     1  \n",
      "3             1             0  La Ca_ada Flintridge     1  \n",
      "4             2             0  La Ca_ada Flintridge     1  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "\n",
      "      CurrentDate  Temp1  Temp2  Temp3  Temp4  avgTemp  Hum1  Hum2  Hum3  \\\n",
      "10340  07/05/2017     31     33     32     25    30.25    33    22    32   \n",
      "\n",
      "       Hum4  ...   Wind3  Wind4  avgWind  Rain  14DayAvgTemp  14dayAvgHum  \\\n",
      "10340    36  ...   5.593  7.457  5.59275     0     28.357143    36.678571   \n",
      "\n",
      "       14DayAvgWind  14DayAvgRain     Location  Fire  \n",
      "10340             5             0  placerville     0  \n",
      "\n",
      "[1 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.io.parsers.read_csv(\n",
    "    'Data/NewBalanced.csv',\n",
    "\n",
    ")\n",
    "\n",
    "print(df.shape)\n",
    "print('\\n')\n",
    "print(df.head(5))\n",
    "print('\\n')\n",
    "print(df.tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file has 23 features and 10,341 data points. Clearly not all of these features are useful for training a model. For example we have date and location. By applying principal component analysis (https://en.wikipedia.org/wiki/Principal_component_analysis) to our data we decided that the 7 features: 'avgTemp', 'avgWind', '14dayAvgTemp', '14dayAvgHum' and '14DayAvgRain' were most conducive to values in the 'Fire' column for whom a 1 corresponds to their being a fire and a 0 to no fire. Next came model selection. Due to the relatively small amount of data we had relative to the number of useful features we opted to go with a Support Vector Machine based model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Support Vector Machine (or SVM for short) is a supervised ML method that can be used for classification of data. Each data item is plotted in n-dimensional space (where n corresponds to the number of features) and then classification is performed by finding a hyperplane that differentiates the classes of the data well. It does so by finding vectors (data points) belonging to each class and basing the position of the hyperplane upon the position of these vectors. These vectors are known as support vectors hence the name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Data/SVM.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM's work particuarly well when the order of the feature-space is large (as in our case) because as its size increases its more likely that the classes will form distinct clusters allowing for a better fitting hyperplane. In addition having a relatively small amount of data isn't game over as presuming the classes form relatively tight data clusters hyperplanes fitted to larger datasets will still be in a similar place to smaller datasets. It is for these reasons we opted to go with a SVM model to make our predictions. Obviously we are assuming that our relatively small dataset is representative of what datap in the class generally looks like and training on a larger dataset likely wouldnt hurt. This is something we would like to improve our model by doing if provided with the resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A SVM in the context of Wildfires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of our data the classes we are training to classify are given by the 'Fire' column in the CSV file (0 for no fire and 1 for fire). We shall consider only the features we deemed important through PCA when training our model. Construction of the dataFrame and splitting of data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "df = pd.io.parsers.read_csv(\n",
    "    'Data/NewBalanced.csv',\n",
    "    header=None,\n",
    "    skiprows = [0],\n",
    "    usecols=[5,10,15,17,18,19,20,22]\n",
    ")\n",
    "\n",
    "X = df.values[:,:7]\n",
    "\n",
    "y = df.values[:,7]\n",
    "\n",
    "#split the data into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=12345)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to standardise the data. Standardisation is an integral part of preprocessing for an SVM. It ensures all features exist on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "std_scale = preprocessing.StandardScaler().fit(X_train) #allows data to be standardised under the same scale\n",
    "X_train_std = std_scale.transform(X_train)\n",
    "X_test_std = std_scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing an SVM from scratch would be a tedious and tricky process. Luckily Scikit-Learn has already done so by creating a python wrapper for the C++ library LibSVM. LibSVM is a very efficient library for running SVM related tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
    "decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "tol=0.001, verbose=False) \n",
    "\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the input parameters above, the most important are C, class_weight, gamma and kernel. The purpose of C is to decide the trade off between fitting the model to the training set and maintaining a smooth hyperplane. class_weight simply denotes the structure of the training data provided relative to its classes. Our data is balanced hence we have used that here. gamma corresponds to how much influence a single training point has over the fitting of the hyperplane. In this example we have let sklearn select gamma automatically. Finally the kernel is the function that is responsible for finding the mathematical relationship between the independent feature vectors and corresponding classes. In our case we have selected 'rbf' or 'radial based field'. This kernel allows fitting of a non linear hyperplane to the data. This is useful as the relationship between our features (i.e. avgtemp, avghumid etc) and our classes (Fire, No fire) may not necessarily be linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code equates to training the model. Predictions can now be made with the following code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  1.,  0.,  1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In addition an accuracy score can be calculated similarily:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 72.5749274895%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy is: {}%'.format(clf.score(X_test, y_test, sample_weight=None)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy can be tweaked by changing hyper-parameters used to train the model as well as altering the data that is trained upon by means of changing the seed when splitting the data. Our final model obtains an accuracy of x%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about probability though?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When designing a model for a scenario such as a wildfire it would be far more useful to have probability values associated with the classe outcomes predicted. Unfortunately this is not an out of the box functionality of SVM's. Luckily however, it can be achieved by applying a procedure known as Platt scaling (https://en.wikipedia.org/wiki/Platt_scaling). Essentially this approach applies a probability distribution to predicted classes by means of the sigmoid function. In doing so it allows us to associate probabilities of data points being in certain classes. LibSVM, thus by proxy Scikit-Learn has an efficient implementation of this procedure which means it is only a single line to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5684794 ,  0.4315206 ],\n",
       "       [ 0.58296673,  0.41703327],\n",
       "       [ 0.88029603,  0.11970397],\n",
       "       ..., \n",
       "       [ 0.12410921,  0.87589079],\n",
       "       [ 0.8163141 ,  0.1836859 ],\n",
       "       [ 0.11962028,  0.88037972]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions can be made using this model by first retrieving the prediction data values from a CSV, standardising them under the same scale used for the training and then running Scikit-Learn's predict() function as shown earlier. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foredf = pd.io.parsers.read_csv(\n",
    "    'Data/svminput.csv',\n",
    "    header=None,\n",
    "    skiprows = [0],\n",
    "    usecols=[1,2,3,4,5,6,8,9,10,11]\n",
    ")\n",
    "\n",
    "X_forecast = foredf.values[:,3:]\n",
    "X_forecast_std = std_scale.transform(X_forecast)\n",
    "\n",
    "fore_pred = clf.predict(X_forecast_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then opted to append the predictions array above to a pandas dataFrame and compile that data frame as a new CSV 'svmoutput.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forearray = foredf.values.tolist()\n",
    "\n",
    "i = 0\n",
    "\n",
    "for element in forearray:\n",
    "\n",
    "    element.append(fore_pred[i])\n",
    "    #element.append(fore_prob[i][1])\n",
    "\n",
    "    i +=1\n",
    "\n",
    "df = pd.DataFrame(forearray)\n",
    "\n",
    "df.to_csv('Data/svmoutput.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can imagine the generated CSV has the same format as the input CSV with the only exception being the appended prediction column added to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1158, 11)\n",
      "\n",
      "\n",
      "   0          1           2       3   4     5          6          7  8  9  10\n",
      "0  0  37.810696 -122.183232  24.285  49  2.35  19.210625  64.125000  3  1   0\n",
      "1  1  37.810696 -122.183232  19.725  61  3.53  19.210625  64.125000  3  1   0\n",
      "2  2  37.810696 -122.183232  17.725  69  3.85  19.210625  64.125000  3  1   0\n",
      "3  3  37.810696 -122.183232  18.635  67  3.35  19.210625  64.125000  3  1   0\n",
      "4  4  37.810696 -122.183232  19.520  62  2.73  19.210625  64.125000  3  1   0\n",
      "5  5  37.810696 -122.183232  19.560  62  2.76  19.210625  64.125000  3  1   0\n",
      "6  0  38.267458 -114.595831  26.085  26  2.31  23.372187  35.270833  2  2   1\n",
      "7  1  38.267458 -114.595831  24.195  32  4.26  23.372187  35.270833  2  2   1\n",
      "8  2  38.267458 -114.595831  24.155  34  4.10  23.372187  35.270833  2  2   1\n",
      "9  3  38.267458 -114.595831  23.075  43  3.87  23.372187  35.270833  2  2   1\n"
     ]
    }
   ],
   "source": [
    "df = pd.io.parsers.read_csv(\n",
    "    'Data/svmoutput.csv', \n",
    "\n",
    ")\n",
    "\n",
    "print(df.shape)\n",
    "print('\\n')\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our code in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ideas and code snippets above are the basis for our code. We have opted to use an object orientated approach as this lends us several advantages such as clarity and generalisability. Below is an example of a script that utilises our code to process, standardise, train then test a model. It outputs a prediction for every value in the test dataset along with its probability (calculated through Platt scaling) and the correct value. Finally it outputs the overall accuracy the model achieved when making predictions upon the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Flinn Dolman\n",
    "\n",
    "@License: MIT\n",
    "\n",
    "An example script that leverages our code to train a model and make predictions based upon it. Predictions\n",
    "are printed to stdout and then the model used to make the predictions is saved.\n",
    "'''\n",
    "from SVM import SVM\n",
    "from Standardiser import Standardiser\n",
    "\n",
    "\n",
    "def Main():\n",
    "    forecast_loc = 'Data/svminput.csv'\n",
    "    standard_data = Standardiser()\n",
    "    standard_data.initialise()\n",
    "    clf = SVM()\n",
    "    clf.initialise(standard_data.get_std_X_train(),standard_data.get_std_X_test(),standard_data.get_y_train(),standard_data.get_y_test())\n",
    "    print('\\nThese are the predictions: {}\\n'.format(clf.predictions()))\n",
    "    predictions, probs = clf.predictions()\n",
    "    y_test = standard_data.get_y_test()\n",
    "\n",
    "\n",
    "    for i in range(0,len(predictions)-1):\n",
    "        print('Prediction: {}, with probability: {}, correct value: {}'.format(predictions[i],probs[i], y_test[i]))\n",
    "\n",
    "    print('Accuracy is: {}%'.format(clf.accuracy()*100))\n",
    "\n",
    "    fore_Pred, fore_Prob = clf.forecast_Pred(standard_data.loadForecast(forecast_loc))\n",
    "\n",
    "    standard_data.make_CSV(fore_Pred,fore_Prob,'Data/svmoutputnew.csv')\n",
    "\n",
    "    clf.saveModel()\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "\n",
    "    Main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of our code means that all this script is really responsible for is initialisation of objects and the formatting of the predictions, probabilities and correct values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
